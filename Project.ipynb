{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90e2ecc0",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1uAUJGEUzfNj6OsWNAimnYCw7eKaHhMUfU1MTj9YwYw4/edit?usp=sharing), [grading rubric](https://docs.google.com/document/d/1hKuRWqFcIdhOkow3Nljcm7PXzIkoa9c_aHkMKZDxWa0/edit?usp=sharing)_\n",
    "\n",
    "_This scaffolding notebook may be used to help setup your final project. It's **totally optional** whether you make use of this or not._\n",
    "\n",
    "_If you do use this notebook, everything provided is optional as well - you may remove or add prose and code as you wish._\n",
    "\n",
    "_**All code below should be consider \"pseudo-code\" - not functional by itself, and only an outline to help you with your own approach.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b475df",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba0b020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements needed for the project, for example:\n",
    "\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "\n",
    "import bs4\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sqlalchemy as db\n",
    "import geopandas as gpd\n",
    "from keplergl import KeplerGl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3024ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any constants you might need; some have been added for you, and \n",
    "# some you need to fill in\n",
    "\n",
    "TAXI_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "TAXI_ZONES_DIR = \"data/taxi_zones\"\n",
    "TAXI_PARQUET_DIR = \"data/taxi_parquet\"\n",
    "TAXI_ZONES_SHAPEFILE = f\"{TAXI_ZONES_DIR}/taxi_zones.shp\"\n",
    "UBER_CSV = \"data/uber_rides_sample.csv\"\n",
    "WEATHER_CSV_DIR = \"data/weather\"\n",
    "\n",
    "CRS = 4326  # coordinate reference system\n",
    "\n",
    "# (lat, lon)\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "LGA_BOX_COORDS = ((40.763589, -73.891745), (40.778865, -73.854838))\n",
    "JFK_BOX_COORDS = ((40.639263, -73.795642), (40.651376, -73.766264))\n",
    "EWR_BOX_COORDS = ((40.686794, -74.194028), (40.699680, -74.165205))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a94906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the QUERY_DIRECTORY exists\n",
    "try:\n",
    "    os.mkdir(QUERY_DIRECTORY)\n",
    "except Exception as e:\n",
    "    if e.errno == 17:\n",
    "        # the directory already exists\n",
    "        pass\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d476f03",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc64775",
   "metadata": {},
   "source": [
    "### Load Taxi Zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c79461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_taxi_zones(shapefile):\n",
    "    taxi_zones = gpd.read_file(filename=shapefile)\n",
    "    taxi_zones = taxi_zones.to_crs(CRS)\n",
    "    taxi_zones['longitude'] = taxi_zones.centroid.x\n",
    "    taxi_zones['latitude'] = taxi_zones.centroid.y\n",
    "    taxi_zones = taxi_zones[['longitude', 'latitude']]\n",
    "    return taxi_zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84db6f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_coords_for_taxi_zone_id(zone_loc_id, loaded_taxi_zones):\n",
    "    return loaded_taxi_zones.loc[zone_loc_id, \"latitude\"], loaded_taxi_zones.loc[zone_loc_id, \"longitude\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd981c9a",
   "metadata": {},
   "source": [
    "### Calculate distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f7d9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance_with_coords(from_coord, to_coord):\n",
    "    pickup_lat, pickup_long = from_coord.iloc[:,0], from_coord.iloc[:,1]\n",
    "    dropoff_lat, dropoff_long = to_coord.iloc[:,0], to_coord.iloc[:,1]    \n",
    "    return 12742 * (((((dropoff_lat - pickup_lat)/2).map(math.sin))**2 + (pickup_lat.map(math.cos)) * (dropoff_lat.map(math.cos)) * (((dropoff_long - pickup_long)/2).map(math.sin))**2)**0.5).map(math.asin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c677df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance_with_zones(from_zone, to_zone):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d25fea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_distance_column(dataframe):\n",
    "    dataframe['distance'] = calculate_distance_with_coords(dataframe[['pickup_latitude', 'pickup_longitude']], dataframe[['dropoff_latitude', 'dropoff_longitude']])\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f57e0ad",
   "metadata": {},
   "source": [
    "### Process Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cbe61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_urls_from_taxi_page(taxi_page):\n",
    "    response = requests.get(TAXI_URL)\n",
    "    soup = bs4.BeautifulSoup(response.text, \"html.parser\")\n",
    "    urls = soup.select('a[title=\"Yellow Taxi Trip Records\"]')\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec84b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_taxi_parquet_urls(all_urls):\n",
    "    pattern = r\"2009|201[0-4]|2015-0[1-6]\"\n",
    "    parquet_urls = []\n",
    "    \n",
    "    for url in all_urls:\n",
    "        href = url[\"href\"]\n",
    "        if re.search(pattern, href):\n",
    "            parquet_urls.append(href)\n",
    "        \n",
    "    return parquet_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac19df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_month(url, loaded_taxi_zones):\n",
    "    fname = url[url.rfind(\"/\")+1:]\n",
    "    fpath = os.path.join(TAXI_PARQUET_DIR, fname)\n",
    "    if os.path.exists(fpath):\n",
    "        df=pd.read_parquet(fpath)\n",
    "    else:\n",
    "        df = pd.read_parquet(url)\n",
    "    \n",
    "    if 'DOLocationID' in df:\n",
    "        df['pickup_latitude'], df['pickup_longitude'] = lookup_coords_for_taxi_zone_id(df['PULocationID'], loaded_taxi_zones)\n",
    "        df['dropoff_latitude'], df['dropoff_longitude']= lookup_coords_for_taxi_zone_id(df['DOLocationID'], loaded_taxi_zones)\n",
    "    \n",
    "    df.rename(columns={'tpep_pickup_datetime':'pickup_datetime' ,'Start_Lon':'pickup_longitude', 'Start_Lat':'pickup_latitude', 'End_Lon':'dropoff_longitude', 'End_Lat':'dropoff_latitude','Tip_Amt':'tip_amount'}, inplace=True)\n",
    "    df = df[['pickup_datetime', 'pickup_latitude','pickup_longitude', 'dropoff_latitude','dropoff_longitude', 'tip_amount']]\n",
    "        \n",
    "    lower_lat, lower_long = NEW_YORK_BOX_COORDS[0]\n",
    "    upper_lat, upper_long = NEW_YORK_BOX_COORDS[1]\n",
    "    \n",
    "    df = df[(df['tip_amount'] >= 0)]\n",
    "    \n",
    "    df = df[(df['pickup_latitude'] >= lower_lat) & (df['pickup_latitude'] <= upper_lat)]\n",
    "    df = df[(df['pickup_longitude'] >= lower_long) & (df['pickup_longitude'] <= upper_long)]\n",
    "    df = df[(df['dropoff_latitude'] >= lower_lat) & (df['dropoff_latitude'] <= upper_lat)]\n",
    "    df = df[(df['dropoff_longitude'] >= lower_long) & (df['dropoff_longitude'] <= upper_long)]\n",
    "\n",
    "    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "    df[\"pickup_latitude\"] = pd.to_numeric(df[\"pickup_latitude\"], errors = \"coerce\")\n",
    "    df[\"pickup_longitude\"] = pd.to_numeric(df[\"pickup_longitude\"], errors = \"coerce\")\n",
    "    df[\"dropoff_latitude\"] = pd.to_numeric(df[\"dropoff_latitude\"], errors = \"coerce\")\n",
    "    df[\"dropoff_longitude\"] = pd.to_numeric(df[\"dropoff_longitude\"], errors = \"coerce\")\n",
    "    df[\"tip_amount\"] = pd.to_numeric(df[\"tip_amount\"], errors = \"coerce\")\n",
    "    \n",
    "    df = df.dropna().sort_values(by=\"pickup_datetime\")\n",
    "    df.index = range(df.shape[0])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6d762d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_data(parquet_urls):\n",
    "    all_taxi_dataframes = []\n",
    "    loaded_taxi_zones = load_taxi_zones(TAXI_ZONES_SHAPEFILE)\n",
    "    \n",
    "    for parquet_url in parquet_urls:\n",
    "        # maybe: first try to see if you've downloaded this exact\n",
    "        # file already and saved it before trying again\n",
    "        dataframe = get_and_clean_month(parquet_url, loaded_taxi_zones)\n",
    "        add_distance_column(dataframe)\n",
    "        # maybe: if the file hasn't been saved, save it so you can\n",
    "        # avoid re-downloading it if you re-run the function\n",
    "        \n",
    "        all_taxi_dataframes.append(dataframe)\n",
    "        \n",
    "    # create one gigantic dataframe with data from every month needed\n",
    "    taxi_data = pd.contact(all_taxi_dataframes)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9f2b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxi_data():\n",
    "    all_urls = get_all_urls_from_taxi_page(TAXI_URL)\n",
    "    all_parquet_urls = filter_taxi_parquet_urls(all_urls)\n",
    "    taxi_data = get_and_clean_taxi_data(all_parquet_urls)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4db2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data = get_taxi_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea31cf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acf4457",
   "metadata": {},
   "source": [
    "### Processing Uber Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa77ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_uber_data(csv_file):\n",
    "    df = pd.read_csv(UBER_CSV, index_col=[0])\n",
    "    \n",
    "    df = df[['pickup_datetime', 'pickup_latitude','pickup_longitude', 'dropoff_latitude','dropoff_longitude']]\n",
    "        \n",
    "    lower_lat, lower_long = NEW_YORK_BOX_COORDS[0]\n",
    "    upper_lat, upper_long = NEW_YORK_BOX_COORDS[1]\n",
    "    \n",
    "    df = df[(df['pickup_latitude'] >= lower_lat) & (df['pickup_latitude'] <= upper_lat)]\n",
    "    df = df[(df['pickup_longitude'] >= lower_long) & (df['pickup_longitude'] <= upper_long)]\n",
    "    df = df[(df['dropoff_latitude'] >= lower_lat) & (df['dropoff_latitude'] <= upper_lat)]\n",
    "    df = df[(df['dropoff_longitude'] >= lower_long) & (df['dropoff_longitude'] <= upper_long)]\n",
    "\n",
    "    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "    df[\"pickup_latitude\"] = pd.to_numeric(df[\"pickup_latitude\"], errors = \"coerce\")\n",
    "    df[\"pickup_longitude\"] = pd.to_numeric(df[\"pickup_longitude\"], errors = \"coerce\")\n",
    "    df[\"dropoff_latitude\"] = pd.to_numeric(df[\"dropoff_latitude\"], errors = \"coerce\")\n",
    "    df[\"dropoff_longitude\"] = pd.to_numeric(df[\"dropoff_longitude\"], errors = \"coerce\")\n",
    "    \n",
    "    df = df.dropna().sort_values(by=\"pickup_datetime\")\n",
    "    df.index = range(df.shape[0])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b1f509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber_data():\n",
    "    uber_dataframe = load_and_clean_uber_data(UBER_CSV)\n",
    "    add_distance_column(uber_dataframe)\n",
    "    return uber_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b1e43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data = get_uber_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ae78a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6bccb3",
   "metadata": {},
   "source": [
    "### Processing Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eb93af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_weather_csvs(directory):\n",
    "    filenames = os.listdir(directory)\n",
    "    return [os.path.join(directory, filename) for filename in filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa678d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file):\n",
    "    df = pd.read_csv(csv_file, index_col=[0], low_memory=False)\n",
    "    df = df[[\"DATE\",\"HourlyWindSpeed\",\"HourlyPrecipitation\"]]\n",
    "\n",
    "    df[\"DATE\"] = pd.to_datetime(df[\"DATE\"])\n",
    "    df[\"HourlyPrecipitation\"] = pd.to_numeric(df[\"HourlyPrecipitation\"], errors = \"coerce\")\n",
    "    df[\"HourlyWindSpeed\"] = pd.to_numeric(df[\"HourlyWindSpeed\"], errors = \"coerce\")\n",
    "        \n",
    "    df = df.rename(columns={\"DATE\": \"weather_date\", \"HourlyWindSpeed\":\"wind_speed\", \"HourlyPrecipitation\":\"precipitation\"})\n",
    "    df = df.dropna().sort_values(by=\"weather_date\")\n",
    "    df.index = range(df.shape[0])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f895a87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file):\n",
    "    hourly_df = clean_month_weather_data_hourly(csv_file)\n",
    "    hourly_df['weather_date'] = hourly_df['weather_date'].dt.date\n",
    "    daily_df = hourly_df.groupby([\"weather_date\"], as_index = False).agg({\"precipitation\":\"sum\",\"wind_speed\":\"mean\"})\n",
    "    daily_df['weather_date'] = pd.to_datetime(daily_df['weather_date'])\n",
    "    daily_df = daily_df.dropna().sort_values(by=\"weather_date\")\n",
    "    daily_df.index = range(daily_df.shape[0])\n",
    "    return daily_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70abc3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data():\n",
    "    weather_csv_files = get_all_weather_csvs(WEATHER_CSV_DIR)\n",
    "    \n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "        \n",
    "    for csv_file in weather_csv_files:\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "        \n",
    "    # create two dataframes with hourly & daily data from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "    \n",
    "    return hourly_data, daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329cf373",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data, daily_weather_data = load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1915f9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5577b4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f126345",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data\n",
    "take the sample datasets generated from Part 1, and populating a SQLite database with tables generated from the datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de25b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d583ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using SQL (as opposed to SQLAlchemy), define the commands \n",
    "# to create your 4 tables/dataframes\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hourly_weather (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    weather_date DATE,\n",
    "    wind_speed FLOAT,\n",
    "    precipitation FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS daily_weather (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    weather_date DATE,\n",
    "    wind_speed FLOAT,\n",
    "    precipitation FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS taxi_trips (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    pickup_datetime DATE,\n",
    "    pickup_longitude FLOAT,\n",
    "    pickup_latitude FLOAT,\n",
    "    dropoff_datetime DATE,\n",
    "    dropoff_longitude FLOAT,\n",
    "    dropoff_latitude FLOAT,\n",
    "    tip_amount FLOAT,\n",
    "    distance FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS uber_trips (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    pickup_datetime DATE,\n",
    "    pickup_longitude FLOAT,\n",
    "    pickup_latitude FLOAT,\n",
    "    dropoff_longitude FLOAT,\n",
    "    dropoff_latitude FLOAT,\n",
    "    distance FLOAT\n",
    ");\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccd3c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38948bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tables with the schema files\n",
    "from sqlalchemy import text\n",
    "with engine.connect() as connection:\n",
    "    for query in [HOURLY_WEATHER_SCHEMA, DAILY_WEATHER_SCHEMA, TAXI_TRIPS_SCHEMA, UBER_TRIPS_SCHEMA]:\n",
    "        connection.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ea4bb7",
   "metadata": {},
   "source": [
    "### Add Data to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadab815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(table_to_df_dict):\n",
    "    for tab in table_to_df_dict:\n",
    "        table_to_df_dict[tab].to_sql(tab, engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2c9297",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_weather_data,\n",
    "    \"daily_weather\": daily_weather_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb252ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be0a761",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c6aafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to write the queries to file\n",
    "def write_query_to_file(query, outfile):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1d650d",
   "metadata": {},
   "source": [
    "### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ba10d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1_FILENAME = \"\"\n",
    "\n",
    "QUERY_1 = \"\"\"\n",
    "TODO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab632f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_1).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cef3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, QUERY_1_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013cdbbd",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cbe84e",
   "metadata": {},
   "source": [
    "### Visualization 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f627fc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a more descriptive name for your function\n",
    "def plot_visual_1(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    values = \"...\"  # use the dataframe to pull out values needed to plot\n",
    "    \n",
    "    # you may want to use matplotlib to plot your visualizations;\n",
    "    # there are also many other plot types (other \n",
    "    # than axes.plot) you can use\n",
    "    axes.plot(values, \"...\")\n",
    "    # there are other methods to use to label your axes, to style \n",
    "    # and set up axes labels, etc\n",
    "    axes.set_title(\"Some Descriptive Title\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b37412",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_1():\n",
    "    # Query SQL database for the data needed.\n",
    "    # You can put the data queried into a pandas dataframe, if you wish\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ca7f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_1()\n",
    "plot_visual_1(some_dataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
